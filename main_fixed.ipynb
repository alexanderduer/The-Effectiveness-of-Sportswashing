{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae12232",
   "metadata": {},
   "source": [
    "# Sentiment classification (local, VS Code + Jupyter)\n",
    "\n",
    "This notebook reads a CSV (default: `df_joined_sample.csv`), classifies each row’s text with a **local Ollama model** via **LangChain**, and writes `df_joined_sample_sentiment.csv`.\n",
    "\n",
    "**Prereqs (outside notebook):**\n",
    "- Install Ollama and pull a model, e.g.: `ollama pull gpt-oss:20b`\n",
    "- Create/activate a Python venv in this folder, then install: `pip install -U pandas numpy langchain-ollama langchain-core`\n",
    "\n",
    "If you change the model name, edit `MODEL_NAME` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0208f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & configuration ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Model + data config\n",
    "MODEL_NAME = \"gpt-oss:20b\"   # e.g. \"llama3.1\" gpt-oss:20b\n",
    "INPUT_CSV  = Path(\"df_joined_sample.csv\")\n",
    "OUTPUT_CSV = Path(\"df_joined_sample_sentiment.csv\")\n",
    "\n",
    "\n",
    "# Quick sanity check: does the input file exist?\n",
    "if not INPUT_CSV.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Can't find {INPUT_CSV.resolve()}\\n\"\n",
    "        \"Put df_joined_sample.csv in the same folder as this notebook, or change INPUT_CSV.\"\n",
    "    )\n",
    "\n",
    "# Optional: if your Ollama server is not the default, you can set:\n",
    "# import os\n",
    "# os.environ['OLLAMA_HOST'] = 'http://127.0.0.1:11434'  # default is usually this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1b74ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 18),\n",
       " 'text',\n",
       " ['text',\n",
       "  'source',\n",
       "  'source_clean',\n",
       "  'doc_id',\n",
       "  'words',\n",
       "  'doc_date',\n",
       "  'title',\n",
       "  'par_id',\n",
       "  'doc_date1',\n",
       "  'date_clean',\n",
       "  'date_clean1',\n",
       "  'outside_range',\n",
       "  'mentions_france_paris',\n",
       "  'Name',\n",
       "  'Country',\n",
       "  'Continent',\n",
       "  'Type_newspaper',\n",
       "  'random_order'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "df = pd.read_csv(INPUT_CSV, encoding=\"latin1\")\n",
    "df = df[(df[\"random_order\"].between(1, 50))] \\\n",
    "       .sort_values(\"random_order\")\n",
    "\n",
    "# Try to locate a text column\n",
    "if \"txt\" in df.columns:\n",
    "    TEXT_COL = \"txt\"\n",
    "else:\n",
    "    # fallback: pick the first object/string-like column\n",
    "    text_candidates = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if not text_candidates:\n",
    "        raise ValueError(\n",
    "            f\"No obvious text column found. Columns are: {list(df.columns)}\\n\"\n",
    "            \"Rename your text column to 'txt' or edit TEXT_COL logic.\"\n",
    "        )\n",
    "    TEXT_COL = text_candidates[0]\n",
    "\n",
    "# Ensure we have a stable row id / progress counter\n",
    "if \"random_order\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={\"index\": \"random_order\"})\n",
    "\n",
    "df_joined_sample = df.copy()\n",
    "\n",
    "df_joined_sample.shape, TEXT_COL, df_joined_sample.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da83392",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (8.0 GiB) than is available (6.1 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m chain = prompt | model\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Quick connectivity test:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe government welcomed the decision and praised the team for its excellent work.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3143\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3141\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3142\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3143\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:373\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m     **kwargs: Any,\n\u001b[32m    370\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    371\u001b[39m     config = ensure_config(config)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    384\u001b[39m         .text\n\u001b[32m    385\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     **kwargs: Any,\n\u001b[32m    782\u001b[39m ) -> LLMResult:\n\u001b[32m    783\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1006\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    988\u001b[39m     run_managers = [\n\u001b[32m    989\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    990\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         )\n\u001b[32m   1005\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1014\u001b[39m     run_managers = [\n\u001b[32m   1015\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1016\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1024\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:810\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    800\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    801\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    806\u001b[39m     **kwargs: Any,\n\u001b[32m    807\u001b[39m ) -> LLMResult:\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    809\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    814\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    817\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    818\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    819\u001b[39m         )\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    821\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_ollama\\llms.py:456\u001b[39m, in \u001b[36mOllamaLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m generations = []\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m     generations.append([final_chunk])\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_ollama\\llms.py:415\u001b[39m, in \u001b[36mOllamaLLM._stream_with_aggregation\u001b[39m\u001b[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    414\u001b[39m thinking_content = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_ollama\\llms.py:359\u001b[39m, in \u001b[36mOllamaLLM._create_generate_stream\u001b[39m\u001b[34m(self, prompt, stop, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_generate_stream\u001b[39m(\n\u001b[32m    353\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    354\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    355\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    356\u001b[39m     **kwargs: Any,\n\u001b[32m    357\u001b[39m ) -> Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.generate(\n\u001b[32m    360\u001b[39m             **\u001b[38;5;28mself\u001b[39m._generate_params(prompt, stop=stop, **kwargs)\n\u001b[32m    361\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ollama\\_client.py:179\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines():\n\u001b[32m    182\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (8.0 GiB) than is available (6.1 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "# --- Build prompt + model chain ---\n",
    "system_prompt = \"\"\"You are a professional sentiment analysis system.\n",
    "\n",
    "Your task: classify the emotional tone of the paragraph below by **language/tone only**.\n",
    "Do NOT infer sentiment from the *event* being described (wins, losses, crises, success, etc.).\n",
    "Focus on evaluative wording (approval/criticism), emotional tone, and affect.\n",
    "\n",
    "Labels:\n",
    "- positive: enthusiasm, approval, joy, relief, celebration, admiration, uplifting tone\n",
    "- negative: criticism, disappointment, concern, frustration, anger, tension, discouraging tone\n",
    "- neutral: mostly factual/descriptive/balanced/flat, no clear evaluative language\n",
    "\n",
    "Output rules:\n",
    "- Reply with exactly ONE word: positive, neutral, or negative.\n",
    "- No punctuation, no explanation, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Paragraph:\n",
    "{text}\n",
    "\n",
    "Label:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt),\n",
    "])\n",
    "\n",
    "# OllamaLLM talks to the local Ollama server (default http://127.0.0.1:11434)\n",
    "model = OllamaLLM(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# Quick connectivity test:\n",
    "print(chain.invoke({\"text\": \"The government welcomed the decision and praised the team for its excellent work.\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0cff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classifier helper ---\n",
    "VALID = {\"positive\", \"neutral\", \"negative\"}\n",
    "\n",
    "def classify_sentiment(text, retries=3, sleep_seconds=1.5):\n",
    "    if pd.isna(text):\n",
    "        return \"neutral\"\n",
    "    text = str(text)\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            out = chain.invoke({\"text\": text})\n",
    "            out = out.strip().lower()\n",
    "\n",
    "            # If the model adds extra words, extract the first valid label we see\n",
    "            m = re.search(r\"\\b(positive|neutral|negative)\\b\", out)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "\n",
    "            return \"error\"\n",
    "        except Exception:\n",
    "            if attempt == retries:\n",
    "                return \"error\"\n",
    "            time.sleep(sleep_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137422d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run classification ---\n",
    "results = []\n",
    "max_order = df_joined_sample[\"random_order\"].max()\n",
    "\n",
    "for _, row in df_joined_sample.iterrows():\n",
    "    paragraph = row[TEXT_COL]\n",
    "    sentiment = classify_sentiment(paragraph)\n",
    "    results.append(sentiment)\n",
    "    print(f\"{row['random_order']}/{max_order} → {sentiment}\")\n",
    "\n",
    "df_joined_sample[\"sentiment\"] = results\n",
    "df_joined_sample[[\"random_order\", TEXT_COL, \"sentiment\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74ff828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\duera\\Downloads\\df_joined_sample_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save output ---\n",
    "df_joined_sample.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Wrote: {OUTPUT_CSV.resolve()}\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
